# -*- coding: utf-8 -*-
"""model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12mVhmwEaCyT1on41cqQo2l4GDYePYWh1
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, GPT2Tokenizer, GPT2LMHeadModel, AdamW
from torch.utils.data import DataLoader, Dataset
import torch
from tqdm import tqdm
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder

class CustomModel:
    def __init__(self):
        # Load your dataset
        merged_df = pd.read_csv('/content/Custom GPT for KIIT - Sheet1 (1).csv')
        text_column = 'User_Input'
        response_column = 'Desired_Model_Response'
        label_column = 'Label'

        # Split the data into training and testing sets
        train_df, test_df = train_test_split(merged_df, test_size=0.2, random_state=42)

        # Tokenize and encode the text data using DistilBERT tokenizer
        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
        train_encodings = tokenizer(train_df[text_column].tolist(), return_tensors='pt', padding=True, truncation=True, max_length=512)
        test_encodings = tokenizer(test_df[text_column].tolist(), return_tensors='pt', padding=True, truncation=True, max_length=512)

        # Create DataLoader for training and testing
        label_encoder = LabelEncoder()
        all_labels = train_df[label_column].tolist() + test_df[label_column].tolist()
        label_encoder.fit(all_labels)

        train_labels = label_encoder.transform(train_df[label_column].tolist())
        test_labels = label_encoder.transform(test_df[label_column].tolist())

        train_dataset = CustomDataset(train_encodings['input_ids'], torch.tensor(train_labels))
        test_dataset = CustomDataset(test_encodings['input_ids'], torch.tensor(test_labels))

        self.train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
        self.test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

        # Load pre-trained DistilBERT model for sequence classification
        self.model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))
        self.optimizer = AdamW(self.model.parameters(), lr=5e-5)  # Experiment with learning rate

        # Load pre-trained GPT-2 model for text generation
        self.gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
        self.gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')
        self.gpt_tokenizer.pad_token = self.gpt_tokenizer.eos_token

        # Device configuration
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        self.gpt_model.to(self.device)

    def train(self, num_epochs=5):
        for epoch in range(num_epochs):
            self.model.train()
            for batch in tqdm(self.train_loader, desc=f'Epoch {epoch}'):
                inputs = batch['text'].to(self.device)
                labels = batch['label'].to(self.device)
                self.optimizer.zero_grad()

                outputs = self.model(inputs, labels=labels)

                loss = outputs.loss
                loss.backward()
                self.optimizer.step()

            # Evaluate the model after each epoch
            accuracy = self.evaluate()
            print(f'Accuracy after Epoch {epoch + 1}: {accuracy}')

    def evaluate(self):
        self.model.eval()
        all_preds = []
        all_labels = []
        for batch in tqdm(self.test_loader, desc='Evaluating'):
            inputs = batch['text'].to(self.device)
            labels = batch['label'].to(self.device)
            with torch.no_grad():
                outputs = self.model(inputs, labels=labels)

            logits = outputs.logits
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())

        # Calculate accuracy
        accuracy = accuracy_score(all_labels, all_preds)
        return accuracy

    def generate_responses(self, input_texts):
        responses = []
        for text in input_texts:
            input_ids = self.gpt_tokenizer.encode(text, return_tensors='pt', max_length=512, truncation=True, padding=True)
            generated_text_ids = self.gpt_model.generate(input_ids.to(self.device), max_length=50, num_return_sequences=1, pad_token_id=self.gpt_tokenizer.eos_token_id)
            generated_text = self.gpt_tokenizer.decode(generated_text_ids[0], skip_special_tokens=True)
            responses.append(generated_text)
        return responses